{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e519f53e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting ollama\n",
      "  Downloading ollama-0.6.0-py3-none-any.whl.metadata (4.3 kB)\n",
      "Collecting httpx>=0.27 (from ollama)\n",
      "  Using cached httpx-0.28.1-py3-none-any.whl.metadata (7.1 kB)\n",
      "Collecting pydantic>=2.9 (from ollama)\n",
      "  Downloading pydantic-2.12.3-py3-none-any.whl.metadata (87 kB)\n",
      "Collecting anyio (from httpx>=0.27->ollama)\n",
      "  Downloading anyio-4.11.0-py3-none-any.whl.metadata (4.1 kB)\n",
      "Collecting certifi (from httpx>=0.27->ollama)\n",
      "  Downloading certifi-2025.10.5-py3-none-any.whl.metadata (2.5 kB)\n",
      "Collecting httpcore==1.* (from httpx>=0.27->ollama)\n",
      "  Using cached httpcore-1.0.9-py3-none-any.whl.metadata (21 kB)\n",
      "Collecting idna (from httpx>=0.27->ollama)\n",
      "  Downloading idna-3.11-py3-none-any.whl.metadata (8.4 kB)\n",
      "Collecting h11>=0.16 (from httpcore==1.*->httpx>=0.27->ollama)\n",
      "  Using cached h11-0.16.0-py3-none-any.whl.metadata (8.3 kB)\n",
      "Collecting annotated-types>=0.6.0 (from pydantic>=2.9->ollama)\n",
      "  Using cached annotated_types-0.7.0-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting pydantic-core==2.41.4 (from pydantic>=2.9->ollama)\n",
      "  Downloading pydantic_core-2.41.4-cp313-cp313-win_amd64.whl.metadata (7.4 kB)\n",
      "Collecting typing-extensions>=4.14.1 (from pydantic>=2.9->ollama)\n",
      "  Using cached typing_extensions-4.15.0-py3-none-any.whl.metadata (3.3 kB)\n",
      "Collecting typing-inspection>=0.4.2 (from pydantic>=2.9->ollama)\n",
      "  Downloading typing_inspection-0.4.2-py3-none-any.whl.metadata (2.6 kB)\n",
      "Collecting sniffio>=1.1 (from anyio->httpx>=0.27->ollama)\n",
      "  Using cached sniffio-1.3.1-py3-none-any.whl.metadata (3.9 kB)\n",
      "Downloading ollama-0.6.0-py3-none-any.whl (14 kB)\n",
      "Using cached httpx-0.28.1-py3-none-any.whl (73 kB)\n",
      "Using cached httpcore-1.0.9-py3-none-any.whl (78 kB)\n",
      "Using cached h11-0.16.0-py3-none-any.whl (37 kB)\n",
      "Downloading pydantic-2.12.3-py3-none-any.whl (462 kB)\n",
      "Downloading pydantic_core-2.41.4-cp313-cp313-win_amd64.whl (2.0 MB)\n",
      "   ---------------------------------------- 0.0/2.0 MB ? eta -:--:--\n",
      "   ----- ---------------------------------- 0.3/2.0 MB ? eta -:--:--\n",
      "   -------------------- ------------------- 1.0/2.0 MB 2.5 MB/s eta 0:00:01\n",
      "   -------------------- ------------------- 1.0/2.0 MB 2.5 MB/s eta 0:00:01\n",
      "   ------------------------- -------------- 1.3/2.0 MB 1.8 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 1.6/2.0 MB 1.4 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 1.6/2.0 MB 1.4 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 1.8/2.0 MB 1.2 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 1.8/2.0 MB 1.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 2.0/2.0 MB 1.1 MB/s  0:00:01\n",
      "Using cached annotated_types-0.7.0-py3-none-any.whl (13 kB)\n",
      "Using cached typing_extensions-4.15.0-py3-none-any.whl (44 kB)\n",
      "Downloading typing_inspection-0.4.2-py3-none-any.whl (14 kB)\n",
      "Downloading anyio-4.11.0-py3-none-any.whl (109 kB)\n",
      "Downloading idna-3.11-py3-none-any.whl (71 kB)\n",
      "Using cached sniffio-1.3.1-py3-none-any.whl (10 kB)\n",
      "Downloading certifi-2025.10.5-py3-none-any.whl (163 kB)\n",
      "Installing collected packages: typing-extensions, sniffio, idna, h11, certifi, annotated-types, typing-inspection, pydantic-core, httpcore, anyio, pydantic, httpx, ollama\n",
      "\n",
      "   ----------------------------------------  0/13 [typing-extensions]\n",
      "   --- ------------------------------------  1/13 [sniffio]\n",
      "   --- ------------------------------------  1/13 [sniffio]\n",
      "   ------ ---------------------------------  2/13 [idna]\n",
      "   ------ ---------------------------------  2/13 [idna]\n",
      "   ------ ---------------------------------  2/13 [idna]\n",
      "   --------- ------------------------------  3/13 [h11]\n",
      "   --------- ------------------------------  3/13 [h11]\n",
      "   ------------ ---------------------------  4/13 [certifi]\n",
      "   ------------ ---------------------------  4/13 [certifi]\n",
      "   --------------- ------------------------  5/13 [annotated-types]\n",
      "   ------------------ ---------------------  6/13 [typing-inspection]\n",
      "   --------------------- ------------------  7/13 [pydantic-core]\n",
      "   --------------------- ------------------  7/13 [pydantic-core]\n",
      "   ------------------------ ---------------  8/13 [httpcore]\n",
      "   ------------------------ ---------------  8/13 [httpcore]\n",
      "   ------------------------ ---------------  8/13 [httpcore]\n",
      "   ------------------------ ---------------  8/13 [httpcore]\n",
      "   ------------------------ ---------------  8/13 [httpcore]\n",
      "   ------------------------ ---------------  8/13 [httpcore]\n",
      "   --------------------------- ------------  9/13 [anyio]\n",
      "   --------------------------- ------------  9/13 [anyio]\n",
      "   --------------------------- ------------  9/13 [anyio]\n",
      "   --------------------------- ------------  9/13 [anyio]\n",
      "   --------------------------- ------------  9/13 [anyio]\n",
      "   --------------------------- ------------  9/13 [anyio]\n",
      "   --------------------------- ------------  9/13 [anyio]\n",
      "   --------------------------- ------------  9/13 [anyio]\n",
      "   --------------------------- ------------  9/13 [anyio]\n",
      "   ------------------------------ --------- 10/13 [pydantic]\n",
      "   ------------------------------ --------- 10/13 [pydantic]\n",
      "   ------------------------------ --------- 10/13 [pydantic]\n",
      "   ------------------------------ --------- 10/13 [pydantic]\n",
      "   ------------------------------ --------- 10/13 [pydantic]\n",
      "   ------------------------------ --------- 10/13 [pydantic]\n",
      "   ------------------------------ --------- 10/13 [pydantic]\n",
      "   ------------------------------ --------- 10/13 [pydantic]\n",
      "   ------------------------------ --------- 10/13 [pydantic]\n",
      "   ------------------------------ --------- 10/13 [pydantic]\n",
      "   ------------------------------ --------- 10/13 [pydantic]\n",
      "   ------------------------------ --------- 10/13 [pydantic]\n",
      "   ------------------------------ --------- 10/13 [pydantic]\n",
      "   ------------------------------ --------- 10/13 [pydantic]\n",
      "   ------------------------------ --------- 10/13 [pydantic]\n",
      "   ------------------------------ --------- 10/13 [pydantic]\n",
      "   ------------------------------ --------- 10/13 [pydantic]\n",
      "   ------------------------------ --------- 10/13 [pydantic]\n",
      "   ------------------------------ --------- 10/13 [pydantic]\n",
      "   ------------------------------ --------- 10/13 [pydantic]\n",
      "   ------------------------------ --------- 10/13 [pydantic]\n",
      "   ------------------------------ --------- 10/13 [pydantic]\n",
      "   ------------------------------ --------- 10/13 [pydantic]\n",
      "   --------------------------------- ------ 11/13 [httpx]\n",
      "   --------------------------------- ------ 11/13 [httpx]\n",
      "   --------------------------------- ------ 11/13 [httpx]\n",
      "   --------------------------------- ------ 11/13 [httpx]\n",
      "   --------------------------------- ------ 11/13 [httpx]\n",
      "   --------------------------------- ------ 11/13 [httpx]\n",
      "   --------------------------------- ------ 11/13 [httpx]\n",
      "   --------------------------------- ------ 11/13 [httpx]\n",
      "   ------------------------------------ --- 12/13 [ollama]\n",
      "   ---------------------------------------- 13/13 [ollama]\n",
      "\n",
      "Successfully installed annotated-types-0.7.0 anyio-4.11.0 certifi-2025.10.5 h11-0.16.0 httpcore-1.0.9 httpx-0.28.1 idna-3.11 ollama-0.6.0 pydantic-2.12.3 pydantic-core-2.41.4 sniffio-1.3.1 typing-extensions-4.15.0 typing-inspection-0.4.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install ollama"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f39e2951",
   "metadata": {},
   "source": [
    "# Load PDF and extract text from pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "674993ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting PyPDF2\n",
      "  Downloading pypdf2-3.0.1-py3-none-any.whl.metadata (6.8 kB)\n",
      "Downloading pypdf2-3.0.1-py3-none-any.whl (232 kB)\n",
      "Installing collected packages: PyPDF2\n",
      "Successfully installed PyPDF2-3.0.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install PyPDF2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "987c1036",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c:\\\\Users\\\\Yue Ning\\\\Desktop\\\\MiscLearning\\\\ai_researcher_database'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fda5b939",
   "metadata": {},
   "outputs": [],
   "source": [
    "import PyPDF2\n",
    "import json\n",
    "import time\n",
    "import ollama\n",
    "\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    \"\"\"\n",
    "    Extract text from a PDF file.\n",
    "    \"\"\"\n",
    "    text = \"\"\n",
    "    with open(pdf_path, \"rb\") as file:\n",
    "        reader = PyPDF2.PdfReader(file)\n",
    "        for page in reader.pages:\n",
    "            text += page.extract_text() or \"\"\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "38c96422",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting text from PDF...\n",
      "Summarizing using Ollama...\n",
      "⚠️ Attempt 1: Invalid output, retrying...\n",
      "⚠️ Attempt 2: Invalid output, retrying...\n",
      "⚠️ Attempt 3: Invalid output, retrying...\n",
      "❌ Max retries reached. Returning last output (may be incomplete).\n",
      "\n",
      "--- Summary ---\n",
      "\n",
      "This is a research paper on the topic of heart disease prediction using machine learning algorithms. The paper reviews various studies and techniques used for predicting heart disease, including:\n",
      "\n",
      "1. **ECG signal analysis**: Several studies have used ECG signals to predict heart disease by analyzing features such as heart rate variability, QRS complex, and T-wave morphology.\n",
      "2. **Machine learning algorithms**: Various machine learning algorithms, including neural networks, support vector machines, random forests, and gradient boosting, have been applied to predict heart disease from various datasets.\n",
      "3. **Deep learning models**: Deep learning models, such as convolutional neural networks (CNNs) and recurrent neural networks (RNNs), have also been used for heart disease prediction by analyzing ECG signals, images, and other clinical data.\n",
      "\n",
      "The paper highlights the strengths and limitations of each technique and discusses the challenges associated with heart disease prediction. Some key findings include:\n",
      "\n",
      "1. **High accuracy**: Many studies have reported high accuracy rates (>90%) in predicting heart disease using machine learning algorithms.\n",
      "2. **Feature selection**: Selecting relevant features from ECG signals, images, or other clinical data is crucial for accurate predictions.\n",
      "3. **Data quality**: The quality of the dataset used for training and testing the models significantly affects the performance of the algorithm.\n",
      "\n",
      "The paper also discusses some of the challenges associated with heart disease prediction, including:\n",
      "\n",
      "1. **Variability in datasets**: Datasets from different sources may have varying levels of noise, bias, or missing values.\n",
      "2. **Interpretability**: The lack of interpretability of complex machine learning models can make it difficult to understand why a particular patient is predicted to develop heart disease.\n",
      "\n",
      "Overall, the paper provides an overview of the current state of research in heart disease prediction using machine learning algorithms and highlights the need for further investigation into the challenges associated with this field.\n"
     ]
    }
   ],
   "source": [
    "def is_valid_markdown(summary_text):\n",
    "    \"\"\"\n",
    "    Basic validation: check that required headings exist.\n",
    "    Returns True if all required sections are present.\n",
    "    \"\"\"\n",
    "    required_headings = [\n",
    "        \"# Title\",\n",
    "        \"# Authors\",\n",
    "        \"# Problem Statement\",\n",
    "        \"# Dataset\",\n",
    "        \"# Models and Methods\",\n",
    "        \"# Results Summary\",\n",
    "        \"# Conclusion\",\n",
    "        \"# Keywords\"\n",
    "    ]\n",
    "    for heading in required_headings:\n",
    "        if heading not in summary_text:\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "def summarize(pdf_text, model, max_retries=3, delay=2):\n",
    "    \"\"\"\n",
    "    Send extracted PDF text to local Mistral 3.2 model through Ollama with structured prompt.\n",
    "    \"\"\"\n",
    "    prompt = f\"\"\"\n",
    "You are an expert research assistant specialized in AI and healthcare papers.\n",
    "\n",
    "You will be given the text of a research paper. Extract a detailed structured summary. INCLUDE numbers from the paper.\n",
    "\n",
    "Summarize the research paper using the following Markdown structure:\n",
    "\n",
    "# Title  \n",
    "# Authors  \n",
    "# Problem Statement  \n",
    "# Dataset  \n",
    "- Description  \n",
    "- Source  \n",
    "- Size  \n",
    "- Preprocessing  \n",
    "\n",
    "# Models and Methods  \n",
    "For each model:  \n",
    "- Name  \n",
    "- Type (e.g., CNN, Random Forest, Transformer)  \n",
    "- Architecture details  \n",
    "- Hyperparameters  \n",
    "- Performance metrics (only those mentioned in the paper, e.g., Accuracy, F1-score, ROC-AUC, MAE, etc.)\n",
    "\n",
    "# Results Summary  \n",
    "# Conclusion  \n",
    "# Keywords \n",
    "\n",
    "--- Research Paper Text Starts ---\n",
    "{pdf_text}\n",
    "--- Research Paper Text Ends ---\n",
    "    \"\"\"\n",
    "\n",
    "    attempt = 0\n",
    "    while attempt < max_retries:\n",
    "        response = ollama.chat(\n",
    "            model= model, \n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "            options={\n",
    "                \"temperature\": 0.5\n",
    "            })\n",
    "        summary = response[\"message\"][\"content\"]\n",
    "\n",
    "        if is_valid_markdown(summary):\n",
    "            return summary\n",
    "        else: \n",
    "            print(f\"⚠️ Attempt {attempt+1}: Invalid output, retrying...\")\n",
    "            attempt +=1\n",
    "            time.sleep(delay)\n",
    "    \n",
    "    # If all retries fail, return what we got with a warning\n",
    "    print(\"❌ Max retries reached. Returning last output (may be incomplete).\")\n",
    "    return summary\n",
    "\n",
    "\n",
    "def main():\n",
    "    pdf_path = \"../../Resources/ml_model_cardio_disease_detection.pdf\"\n",
    "\n",
    "    print(\"Extracting text from PDF...\")\n",
    "    pdf_text = extract_text_from_pdf(pdf_path)\n",
    "\n",
    "    print(\"Summarizing using Ollama...\")\n",
    "    summary = summarize(pdf_text, \"llama3.1:latest\")\n",
    "\n",
    "    print(\"\\n--- Summary ---\\n\")\n",
    "    print(summary)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f6ae2467",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting camelot-py\n",
      "  Downloading camelot_py-1.0.9-py3-none-any.whl.metadata (9.8 kB)\n",
      "Collecting click>=8.0.1 (from camelot-py)\n",
      "  Using cached click-8.3.0-py3-none-any.whl.metadata (2.6 kB)\n",
      "Collecting chardet>=5.1.0 (from camelot-py)\n",
      "  Downloading chardet-5.2.0-py3-none-any.whl.metadata (3.4 kB)\n",
      "Requirement already satisfied: numpy>=1.26.1 in c:\\users\\yue ning\\desktop\\misclearning\\ai_researcher_database\\venv\\lib\\site-packages (from camelot-py) (2.3.4)\n",
      "Collecting openpyxl>=3.1.0 (from camelot-py)\n",
      "  Using cached openpyxl-3.1.5-py2.py3-none-any.whl.metadata (2.5 kB)\n",
      "Collecting pdfminer-six>=20240706 (from camelot-py)\n",
      "  Downloading pdfminer_six-20250506-py3-none-any.whl.metadata (4.2 kB)\n",
      "Collecting pypdf<6.0,>=4.0 (from camelot-py)\n",
      "  Downloading pypdf-5.9.0-py3-none-any.whl.metadata (7.1 kB)\n",
      "Requirement already satisfied: pandas>=2.2.2 in c:\\users\\yue ning\\desktop\\misclearning\\ai_researcher_database\\venv\\lib\\site-packages (from camelot-py) (2.3.3)\n",
      "Collecting tabulate>=0.9.0 (from camelot-py)\n",
      "  Downloading tabulate-0.9.0-py3-none-any.whl.metadata (34 kB)\n",
      "Collecting opencv-python-headless>=4.7.0.68 (from camelot-py)\n",
      "  Using cached opencv_python_headless-4.12.0.88-cp37-abi3-win_amd64.whl.metadata (20 kB)\n",
      "Collecting pypdfium2>=4 (from camelot-py)\n",
      "  Downloading pypdfium2-4.30.0-py3-none-win_amd64.whl.metadata (48 kB)\n",
      "Collecting pillow>=10.4.0 (from camelot-py)\n",
      "  Downloading pillow-12.0.0-cp313-cp313-win_amd64.whl.metadata (9.0 kB)\n",
      "Requirement already satisfied: colorama in c:\\users\\yue ning\\desktop\\misclearning\\ai_researcher_database\\venv\\lib\\site-packages (from click>=8.0.1->camelot-py) (0.4.6)\n",
      "Collecting numpy>=1.26.1 (from camelot-py)\n",
      "  Using cached numpy-2.2.6-cp313-cp313-win_amd64.whl.metadata (60 kB)\n",
      "Collecting et-xmlfile (from openpyxl>=3.1.0->camelot-py)\n",
      "  Using cached et_xmlfile-2.0.0-py3-none-any.whl.metadata (2.7 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\yue ning\\desktop\\misclearning\\ai_researcher_database\\venv\\lib\\site-packages (from pandas>=2.2.2->camelot-py) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\yue ning\\desktop\\misclearning\\ai_researcher_database\\venv\\lib\\site-packages (from pandas>=2.2.2->camelot-py) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\yue ning\\desktop\\misclearning\\ai_researcher_database\\venv\\lib\\site-packages (from pandas>=2.2.2->camelot-py) (2025.2)\n",
      "Collecting charset-normalizer>=2.0.0 (from pdfminer-six>=20240706->camelot-py)\n",
      "  Downloading charset_normalizer-3.4.4-cp313-cp313-win_amd64.whl.metadata (38 kB)\n",
      "Collecting cryptography>=36.0.0 (from pdfminer-six>=20240706->camelot-py)\n",
      "  Downloading cryptography-46.0.3-cp311-abi3-win_amd64.whl.metadata (5.7 kB)\n",
      "Collecting cffi>=2.0.0 (from cryptography>=36.0.0->pdfminer-six>=20240706->camelot-py)\n",
      "  Downloading cffi-2.0.0-cp313-cp313-win_amd64.whl.metadata (2.6 kB)\n",
      "Collecting pycparser (from cffi>=2.0.0->cryptography>=36.0.0->pdfminer-six>=20240706->camelot-py)\n",
      "  Downloading pycparser-2.23-py3-none-any.whl.metadata (993 bytes)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\yue ning\\desktop\\misclearning\\ai_researcher_database\\venv\\lib\\site-packages (from python-dateutil>=2.8.2->pandas>=2.2.2->camelot-py) (1.17.0)\n",
      "Downloading camelot_py-1.0.9-py3-none-any.whl (66 kB)\n",
      "Downloading pypdf-5.9.0-py3-none-any.whl (313 kB)\n",
      "Downloading chardet-5.2.0-py3-none-any.whl (199 kB)\n",
      "Using cached click-8.3.0-py3-none-any.whl (107 kB)\n",
      "Using cached opencv_python_headless-4.12.0.88-cp37-abi3-win_amd64.whl (38.9 MB)\n",
      "Using cached numpy-2.2.6-cp313-cp313-win_amd64.whl (12.6 MB)\n",
      "Using cached openpyxl-3.1.5-py2.py3-none-any.whl (250 kB)\n",
      "Downloading pdfminer_six-20250506-py3-none-any.whl (5.6 MB)\n",
      "   ---------------------------------------- 0.0/5.6 MB ? eta -:--:--\n",
      "   ------------- -------------------------- 1.8/5.6 MB 8.9 MB/s eta 0:00:01\n",
      "   --------------------------- ------------ 3.9/5.6 MB 9.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------  5.5/5.6 MB 10.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 5.6/5.6 MB 8.7 MB/s  0:00:00\n",
      "Downloading charset_normalizer-3.4.4-cp313-cp313-win_amd64.whl (107 kB)\n",
      "Downloading cryptography-46.0.3-cp311-abi3-win_amd64.whl (3.5 MB)\n",
      "   ---------------------------------------- 0.0/3.5 MB ? eta -:--:--\n",
      "   ----------------------- ---------------- 2.1/3.5 MB 9.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 3.5/3.5 MB 7.7 MB/s  0:00:00\n",
      "Downloading cffi-2.0.0-cp313-cp313-win_amd64.whl (183 kB)\n",
      "Downloading pillow-12.0.0-cp313-cp313-win_amd64.whl (7.0 MB)\n",
      "   ---------------------------------------- 0.0/7.0 MB ? eta -:--:--\n",
      "   ----------- ---------------------------- 2.1/7.0 MB 10.3 MB/s eta 0:00:01\n",
      "   ----------------------- ---------------- 4.2/7.0 MB 10.4 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 6.8/7.0 MB 10.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 7.0/7.0 MB 9.4 MB/s  0:00:00\n",
      "Downloading pypdfium2-4.30.0-py3-none-win_amd64.whl (2.9 MB)\n",
      "   ---------------------------------------- 0.0/2.9 MB ? eta -:--:--\n",
      "   ----------------------------- ---------- 2.1/2.9 MB 11.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.9/2.9 MB 10.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 2.9/2.9 MB 5.2 MB/s  0:00:00\n",
      "Downloading tabulate-0.9.0-py3-none-any.whl (35 kB)\n",
      "Using cached et_xmlfile-2.0.0-py3-none-any.whl (18 kB)\n",
      "Downloading pycparser-2.23-py3-none-any.whl (118 kB)\n",
      "Installing collected packages: tabulate, pypdfium2, pypdf, pycparser, pillow, numpy, et-xmlfile, click, charset-normalizer, chardet, openpyxl, opencv-python-headless, cffi, cryptography, pdfminer-six, camelot-py\n",
      "\n",
      "   ----------------------------------------  0/16 [tabulate]\n",
      "   -- -------------------------------------  1/16 [pypdfium2]\n",
      "   -- -------------------------------------  1/16 [pypdfium2]\n",
      "   -- -------------------------------------  1/16 [pypdfium2]\n",
      "   -- -------------------------------------  1/16 [pypdfium2]\n",
      "   -- -------------------------------------  1/16 [pypdfium2]\n",
      "   ----- ----------------------------------  2/16 [pypdf]\n",
      "   ----- ----------------------------------  2/16 [pypdf]\n",
      "   ----- ----------------------------------  2/16 [pypdf]\n",
      "   ----- ----------------------------------  2/16 [pypdf]\n",
      "   ----- ----------------------------------  2/16 [pypdf]\n",
      "   ----- ----------------------------------  2/16 [pypdf]\n",
      "   ----- ----------------------------------  2/16 [pypdf]\n",
      "   ----- ----------------------------------  2/16 [pypdf]\n",
      "   ------- --------------------------------  3/16 [pycparser]\n",
      "   ------- --------------------------------  3/16 [pycparser]\n",
      "   ------- --------------------------------  3/16 [pycparser]\n",
      "   ------- --------------------------------  3/16 [pycparser]\n",
      "   ---------- -----------------------------  4/16 [pillow]\n",
      "   ---------- -----------------------------  4/16 [pillow]\n",
      "   ---------- -----------------------------  4/16 [pillow]\n",
      "   ---------- -----------------------------  4/16 [pillow]\n",
      "   ---------- -----------------------------  4/16 [pillow]\n",
      "   ---------- -----------------------------  4/16 [pillow]\n",
      "   ---------- -----------------------------  4/16 [pillow]\n",
      "   ---------- -----------------------------  4/16 [pillow]\n",
      "   ---------- -----------------------------  4/16 [pillow]\n",
      "   ---------- -----------------------------  4/16 [pillow]\n",
      "   ---------- -----------------------------  4/16 [pillow]\n",
      "   ---------- -----------------------------  4/16 [pillow]\n",
      "   ---------- -----------------------------  4/16 [pillow]\n",
      "   ---------- -----------------------------  4/16 [pillow]\n",
      "   ---------- -----------------------------  4/16 [pillow]\n",
      "   ---------- -----------------------------  4/16 [pillow]\n",
      "  Attempting uninstall: numpy\n",
      "   ---------- -----------------------------  4/16 [pillow]\n",
      "    Found existing installation: numpy 2.3.4\n",
      "   ---------- -----------------------------  4/16 [pillow]\n",
      "   ------------ ---------------------------  5/16 [numpy]\n",
      "    Uninstalling numpy-2.3.4:\n",
      "   ------------ ---------------------------  5/16 [numpy]\n",
      "      Successfully uninstalled numpy-2.3.4\n",
      "   ------------ ---------------------------  5/16 [numpy]\n",
      "   ------------ ---------------------------  5/16 [numpy]\n",
      "   ------------ ---------------------------  5/16 [numpy]\n",
      "   ------------ ---------------------------  5/16 [numpy]\n",
      "   ------------ ---------------------------  5/16 [numpy]\n",
      "   ------------ ---------------------------  5/16 [numpy]\n",
      "   ------------ ---------------------------  5/16 [numpy]\n",
      "   ------------ ---------------------------  5/16 [numpy]\n",
      "   ------------ ---------------------------  5/16 [numpy]\n",
      "   ------------ ---------------------------  5/16 [numpy]\n",
      "   ------------ ---------------------------  5/16 [numpy]\n",
      "   ------------ ---------------------------  5/16 [numpy]\n",
      "   ------------ ---------------------------  5/16 [numpy]\n",
      "   ------------ ---------------------------  5/16 [numpy]\n",
      "   ------------ ---------------------------  5/16 [numpy]\n",
      "   ------------ ---------------------------  5/16 [numpy]\n",
      "   ------------ ---------------------------  5/16 [numpy]\n",
      "   ------------ ---------------------------  5/16 [numpy]\n",
      "   ------------ ---------------------------  5/16 [numpy]\n",
      "   ------------ ---------------------------  5/16 [numpy]\n",
      "   ------------ ---------------------------  5/16 [numpy]\n",
      "   ------------ ---------------------------  5/16 [numpy]\n",
      "   ------------ ---------------------------  5/16 [numpy]\n",
      "   ------------ ---------------------------  5/16 [numpy]\n",
      "   ------------ ---------------------------  5/16 [numpy]\n",
      "   ------------ ---------------------------  5/16 [numpy]\n",
      "   ------------ ---------------------------  5/16 [numpy]\n",
      "   ------------ ---------------------------  5/16 [numpy]\n",
      "   ------------ ---------------------------  5/16 [numpy]\n",
      "   ------------ ---------------------------  5/16 [numpy]\n",
      "   ------------ ---------------------------  5/16 [numpy]\n",
      "   ------------ ---------------------------  5/16 [numpy]\n",
      "   ------------ ---------------------------  5/16 [numpy]\n",
      "   ------------ ---------------------------  5/16 [numpy]\n",
      "   ------------ ---------------------------  5/16 [numpy]\n",
      "   ------------ ---------------------------  5/16 [numpy]\n",
      "   ------------ ---------------------------  5/16 [numpy]\n",
      "   ------------ ---------------------------  5/16 [numpy]\n",
      "   ------------ ---------------------------  5/16 [numpy]\n",
      "   ------------ ---------------------------  5/16 [numpy]\n",
      "   ------------ ---------------------------  5/16 [numpy]\n",
      "   ------------ ---------------------------  5/16 [numpy]\n",
      "   ------------ ---------------------------  5/16 [numpy]\n",
      "   ------------ ---------------------------  5/16 [numpy]\n",
      "   ------------ ---------------------------  5/16 [numpy]\n",
      "   ------------ ---------------------------  5/16 [numpy]\n",
      "   ------------ ---------------------------  5/16 [numpy]\n",
      "   ------------ ---------------------------  5/16 [numpy]\n",
      "   ------------ ---------------------------  5/16 [numpy]\n",
      "   ------------ ---------------------------  5/16 [numpy]\n",
      "   ------------ ---------------------------  5/16 [numpy]\n",
      "   ------------ ---------------------------  5/16 [numpy]\n",
      "   ------------ ---------------------------  5/16 [numpy]\n",
      "   ------------ ---------------------------  5/16 [numpy]\n",
      "   ------------ ---------------------------  5/16 [numpy]\n",
      "   ------------ ---------------------------  5/16 [numpy]\n",
      "   ------------ ---------------------------  5/16 [numpy]\n",
      "   ------------ ---------------------------  5/16 [numpy]\n",
      "   ------------ ---------------------------  5/16 [numpy]\n",
      "   ------------ ---------------------------  5/16 [numpy]\n",
      "   ------------ ---------------------------  5/16 [numpy]\n",
      "   ------------ ---------------------------  5/16 [numpy]\n",
      "   ------------ ---------------------------  5/16 [numpy]\n",
      "   ------------ ---------------------------  5/16 [numpy]\n",
      "   ------------ ---------------------------  5/16 [numpy]\n",
      "   ------------ ---------------------------  5/16 [numpy]\n",
      "   ------------ ---------------------------  5/16 [numpy]\n",
      "   ------------ ---------------------------  5/16 [numpy]\n",
      "   ------------ ---------------------------  5/16 [numpy]\n",
      "   ------------ ---------------------------  5/16 [numpy]\n",
      "   ------------ ---------------------------  5/16 [numpy]\n",
      "   ------------ ---------------------------  5/16 [numpy]\n",
      "   ------------ ---------------------------  5/16 [numpy]\n",
      "   ------------ ---------------------------  5/16 [numpy]\n",
      "   ------------ ---------------------------  5/16 [numpy]\n",
      "   ------------ ---------------------------  5/16 [numpy]\n",
      "   ------------ ---------------------------  5/16 [numpy]\n",
      "   ------------ ---------------------------  5/16 [numpy]\n",
      "   ------------ ---------------------------  5/16 [numpy]\n",
      "   ------------ ---------------------------  5/16 [numpy]\n",
      "   ------------ ---------------------------  5/16 [numpy]\n",
      "   ------------ ---------------------------  5/16 [numpy]\n",
      "   ------------ ---------------------------  5/16 [numpy]\n",
      "   ------------ ---------------------------  5/16 [numpy]\n",
      "   ------------ ---------------------------  5/16 [numpy]\n",
      "   ------------ ---------------------------  5/16 [numpy]\n",
      "   ------------ ---------------------------  5/16 [numpy]\n",
      "   ------------ ---------------------------  5/16 [numpy]\n",
      "   ------------ ---------------------------  5/16 [numpy]\n",
      "   ------------ ---------------------------  5/16 [numpy]\n",
      "   ------------ ---------------------------  5/16 [numpy]\n",
      "   ------------ ---------------------------  5/16 [numpy]\n",
      "   ------------ ---------------------------  5/16 [numpy]\n",
      "   ------------ ---------------------------  5/16 [numpy]\n",
      "   ------------ ---------------------------  5/16 [numpy]\n",
      "   ------------ ---------------------------  5/16 [numpy]\n",
      "   ------------ ---------------------------  5/16 [numpy]\n",
      "   ------------ ---------------------------  5/16 [numpy]\n",
      "   ------------ ---------------------------  5/16 [numpy]\n",
      "   ------------ ---------------------------  5/16 [numpy]\n",
      "   ------------ ---------------------------  5/16 [numpy]\n",
      "   ------------ ---------------------------  5/16 [numpy]\n",
      "   ------------ ---------------------------  5/16 [numpy]\n",
      "   ------------ ---------------------------  5/16 [numpy]\n",
      "   ------------ ---------------------------  5/16 [numpy]\n",
      "   ------------ ---------------------------  5/16 [numpy]\n",
      "   ------------ ---------------------------  5/16 [numpy]\n",
      "   ------------ ---------------------------  5/16 [numpy]\n",
      "   ------------ ---------------------------  5/16 [numpy]\n",
      "   ------------ ---------------------------  5/16 [numpy]\n",
      "   ------------ ---------------------------  5/16 [numpy]\n",
      "   ------------ ---------------------------  5/16 [numpy]\n",
      "   ------------ ---------------------------  5/16 [numpy]\n",
      "   ------------ ---------------------------  5/16 [numpy]\n",
      "   ------------ ---------------------------  5/16 [numpy]\n",
      "   ------------ ---------------------------  5/16 [numpy]\n",
      "   ------------ ---------------------------  5/16 [numpy]\n",
      "   ------------ ---------------------------  5/16 [numpy]\n",
      "   ------------ ---------------------------  5/16 [numpy]\n",
      "   ------------ ---------------------------  5/16 [numpy]\n",
      "   ------------ ---------------------------  5/16 [numpy]\n",
      "   ------------ ---------------------------  5/16 [numpy]\n",
      "   ------------ ---------------------------  5/16 [numpy]\n",
      "   ------------ ---------------------------  5/16 [numpy]\n",
      "   ------------ ---------------------------  5/16 [numpy]\n",
      "   ------------ ---------------------------  5/16 [numpy]\n",
      "   ------------ ---------------------------  5/16 [numpy]\n",
      "   ------------ ---------------------------  5/16 [numpy]\n",
      "   ------------ ---------------------------  5/16 [numpy]\n",
      "   ------------ ---------------------------  5/16 [numpy]\n",
      "   ------------ ---------------------------  5/16 [numpy]\n",
      "   ------------ ---------------------------  5/16 [numpy]\n",
      "   ------------ ---------------------------  5/16 [numpy]\n",
      "   ------------ ---------------------------  5/16 [numpy]\n",
      "   ------------ ---------------------------  5/16 [numpy]\n",
      "   ------------ ---------------------------  5/16 [numpy]\n",
      "   ------------ ---------------------------  5/16 [numpy]\n",
      "   ------------ ---------------------------  5/16 [numpy]\n",
      "   ------------ ---------------------------  5/16 [numpy]\n",
      "   --------------- ------------------------  6/16 [et-xmlfile]\n",
      "   ----------------- ----------------------  7/16 [click]\n",
      "   ----------------- ----------------------  7/16 [click]\n",
      "   ----------------- ----------------------  7/16 [click]\n",
      "   -------------------- -------------------  8/16 [charset-normalizer]\n",
      "   -------------------- -------------------  8/16 [charset-normalizer]\n",
      "   -------------------- -------------------  8/16 [charset-normalizer]\n",
      "   ---------------------- -----------------  9/16 [chardet]\n",
      "   ---------------------- -----------------  9/16 [chardet]\n",
      "   ---------------------- -----------------  9/16 [chardet]\n",
      "   ---------------------- -----------------  9/16 [chardet]\n",
      "   ---------------------- -----------------  9/16 [chardet]\n",
      "   ---------------------- -----------------  9/16 [chardet]\n",
      "   ---------------------- -----------------  9/16 [chardet]\n",
      "   ------------------------- -------------- 10/16 [openpyxl]\n",
      "   ------------------------- -------------- 10/16 [openpyxl]\n",
      "   ------------------------- -------------- 10/16 [openpyxl]\n",
      "   ------------------------- -------------- 10/16 [openpyxl]\n",
      "   ------------------------- -------------- 10/16 [openpyxl]\n",
      "   ------------------------- -------------- 10/16 [openpyxl]\n",
      "   ------------------------- -------------- 10/16 [openpyxl]\n",
      "   ------------------------- -------------- 10/16 [openpyxl]\n",
      "   ------------------------- -------------- 10/16 [openpyxl]\n",
      "   ------------------------- -------------- 10/16 [openpyxl]\n",
      "   ------------------------- -------------- 10/16 [openpyxl]\n",
      "   ------------------------- -------------- 10/16 [openpyxl]\n",
      "   ------------------------- -------------- 10/16 [openpyxl]\n",
      "   ------------------------- -------------- 10/16 [openpyxl]\n",
      "   ------------------------- -------------- 10/16 [openpyxl]\n",
      "   ------------------------- -------------- 10/16 [openpyxl]\n",
      "   ------------------------- -------------- 10/16 [openpyxl]\n",
      "   ------------------------- -------------- 10/16 [openpyxl]\n",
      "   ------------------------- -------------- 10/16 [openpyxl]\n",
      "   ------------------------- -------------- 10/16 [openpyxl]\n",
      "   ------------------------- -------------- 10/16 [openpyxl]\n",
      "   ------------------------- -------------- 10/16 [openpyxl]\n",
      "   ------------------------- -------------- 10/16 [openpyxl]\n",
      "   ------------------------- -------------- 10/16 [openpyxl]\n",
      "   --------------------------- ------------ 11/16 [opencv-python-headless]\n",
      "   --------------------------- ------------ 11/16 [opencv-python-headless]\n",
      "   --------------------------- ------------ 11/16 [opencv-python-headless]\n",
      "   --------------------------- ------------ 11/16 [opencv-python-headless]\n",
      "   --------------------------- ------------ 11/16 [opencv-python-headless]\n",
      "   --------------------------- ------------ 11/16 [opencv-python-headless]\n",
      "   --------------------------- ------------ 11/16 [opencv-python-headless]\n",
      "   --------------------------- ------------ 11/16 [opencv-python-headless]\n",
      "   --------------------------- ------------ 11/16 [opencv-python-headless]\n",
      "   --------------------------- ------------ 11/16 [opencv-python-headless]\n",
      "   ------------------------------ --------- 12/16 [cffi]\n",
      "   ------------------------------ --------- 12/16 [cffi]\n",
      "   ------------------------------ --------- 12/16 [cffi]\n",
      "   ------------------------------ --------- 12/16 [cffi]\n",
      "   -------------------------------- ------- 13/16 [cryptography]\n",
      "   -------------------------------- ------- 13/16 [cryptography]\n",
      "   -------------------------------- ------- 13/16 [cryptography]\n",
      "   -------------------------------- ------- 13/16 [cryptography]\n",
      "   -------------------------------- ------- 13/16 [cryptography]\n",
      "   -------------------------------- ------- 13/16 [cryptography]\n",
      "   -------------------------------- ------- 13/16 [cryptography]\n",
      "   -------------------------------- ------- 13/16 [cryptography]\n",
      "   -------------------------------- ------- 13/16 [cryptography]\n",
      "   -------------------------------- ------- 13/16 [cryptography]\n",
      "   -------------------------------- ------- 13/16 [cryptography]\n",
      "   -------------------------------- ------- 13/16 [cryptography]\n",
      "   -------------------------------- ------- 13/16 [cryptography]\n",
      "   -------------------------------- ------- 13/16 [cryptography]\n",
      "   -------------------------------- ------- 13/16 [cryptography]\n",
      "   ----------------------------------- ---- 14/16 [pdfminer-six]\n",
      "   ----------------------------------- ---- 14/16 [pdfminer-six]\n",
      "   ----------------------------------- ---- 14/16 [pdfminer-six]\n",
      "   ----------------------------------- ---- 14/16 [pdfminer-six]\n",
      "   ----------------------------------- ---- 14/16 [pdfminer-six]\n",
      "   ----------------------------------- ---- 14/16 [pdfminer-six]\n",
      "   ----------------------------------- ---- 14/16 [pdfminer-six]\n",
      "   ----------------------------------- ---- 14/16 [pdfminer-six]\n",
      "   ----------------------------------- ---- 14/16 [pdfminer-six]\n",
      "   ----------------------------------- ---- 14/16 [pdfminer-six]\n",
      "   ----------------------------------- ---- 14/16 [pdfminer-six]\n",
      "   ----------------------------------- ---- 14/16 [pdfminer-six]\n",
      "   ------------------------------------- -- 15/16 [camelot-py]\n",
      "   ------------------------------------- -- 15/16 [camelot-py]\n",
      "   ------------------------------------- -- 15/16 [camelot-py]\n",
      "   ---------------------------------------- 16/16 [camelot-py]\n",
      "\n",
      "Successfully installed camelot-py-1.0.9 cffi-2.0.0 chardet-5.2.0 charset-normalizer-3.4.4 click-8.3.0 cryptography-46.0.3 et-xmlfile-2.0.0 numpy-2.2.6 opencv-python-headless-4.12.0.88 openpyxl-3.1.5 pdfminer-six-20250506 pillow-12.0.0 pycparser-2.23 pypdf-5.9.0 pypdfium2-4.30.0 tabulate-0.9.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  WARNING: Failed to remove contents in a temporary directory 'C:\\Users\\Yue Ning\\Desktop\\MiscLearning\\ai_researcher_database\\venv\\Lib\\site-packages\\~umpy.libs'.\n",
      "  You can safely remove it manually.\n",
      "  WARNING: Failed to remove contents in a temporary directory 'C:\\Users\\Yue Ning\\Desktop\\MiscLearning\\ai_researcher_database\\venv\\Lib\\site-packages\\~umpy'.\n",
      "  You can safely remove it manually.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting ghostscript\n",
      "  Downloading ghostscript-0.8.1-py3-none-any.whl.metadata (4.4 kB)\n",
      "Downloading ghostscript-0.8.1-py3-none-any.whl (25 kB)\n",
      "Installing collected packages: ghostscript\n",
      "Successfully installed ghostscript-0.8.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Collecting promptlayer\n",
      "  Downloading promptlayer-1.0.71-py3-none-any.whl.metadata (4.9 kB)\n",
      "Collecting ably<3.0.0,>=2.0.11 (from promptlayer)\n",
      "  Downloading ably-2.1.1-py3-none-any.whl.metadata (2.5 kB)\n",
      "Collecting aiohttp<4.0.0,>=3.10.10 (from promptlayer)\n",
      "  Downloading aiohttp-3.13.1-cp313-cp313-win_amd64.whl.metadata (8.4 kB)\n",
      "Requirement already satisfied: httpx<0.29.0,>=0.28.1 in c:\\users\\yue ning\\desktop\\misclearning\\ai_researcher_database\\venv\\lib\\site-packages (from promptlayer) (0.28.1)\n",
      "Requirement already satisfied: nest-asyncio<2.0.0,>=1.6.0 in c:\\users\\yue ning\\desktop\\misclearning\\ai_researcher_database\\venv\\lib\\site-packages (from promptlayer) (1.6.0)\n",
      "Collecting opentelemetry-api<2.0.0,>=1.26.0 (from promptlayer)\n",
      "  Downloading opentelemetry_api-1.38.0-py3-none-any.whl.metadata (1.5 kB)\n",
      "Collecting opentelemetry-sdk<2.0.0,>=1.26.0 (from promptlayer)\n",
      "  Downloading opentelemetry_sdk-1.38.0-py3-none-any.whl.metadata (1.5 kB)\n",
      "Collecting requests<3.0.0,>=2.31.0 (from promptlayer)\n",
      "  Downloading requests-2.32.5-py3-none-any.whl.metadata (4.9 kB)\n",
      "Collecting h2<5.0.0,>=4.1.0 (from ably<3.0.0,>=2.0.11->promptlayer)\n",
      "  Using cached h2-4.3.0-py3-none-any.whl.metadata (5.1 kB)\n",
      "Collecting methoddispatch<4.0.0,>=3.0.2 (from ably<3.0.0,>=2.0.11->promptlayer)\n",
      "  Downloading methoddispatch-3.0.2-py2.py3-none-any.whl.metadata (7.2 kB)\n",
      "Collecting msgpack<2.0.0,>=1.0.0 (from ably<3.0.0,>=2.0.11->promptlayer)\n",
      "  Downloading msgpack-1.1.2-cp313-cp313-win_amd64.whl.metadata (8.4 kB)\n",
      "Collecting pyee<13.0.0,>=11.1.0 (from ably<3.0.0,>=2.0.11->promptlayer)\n",
      "  Downloading pyee-12.1.1-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting websockets<16.0,>=15.0 (from ably<3.0.0,>=2.0.11->promptlayer)\n",
      "  Using cached websockets-15.0.1-cp313-cp313-win_amd64.whl.metadata (7.0 kB)\n",
      "Collecting aiohappyeyeballs>=2.5.0 (from aiohttp<4.0.0,>=3.10.10->promptlayer)\n",
      "  Downloading aiohappyeyeballs-2.6.1-py3-none-any.whl.metadata (5.9 kB)\n",
      "Collecting aiosignal>=1.4.0 (from aiohttp<4.0.0,>=3.10.10->promptlayer)\n",
      "  Downloading aiosignal-1.4.0-py3-none-any.whl.metadata (3.7 kB)\n",
      "Collecting attrs>=17.3.0 (from aiohttp<4.0.0,>=3.10.10->promptlayer)\n",
      "  Downloading attrs-25.4.0-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting frozenlist>=1.1.1 (from aiohttp<4.0.0,>=3.10.10->promptlayer)\n",
      "  Downloading frozenlist-1.8.0-cp313-cp313-win_amd64.whl.metadata (21 kB)\n",
      "Collecting multidict<7.0,>=4.5 (from aiohttp<4.0.0,>=3.10.10->promptlayer)\n",
      "  Downloading multidict-6.7.0-cp313-cp313-win_amd64.whl.metadata (5.5 kB)\n",
      "Collecting propcache>=0.2.0 (from aiohttp<4.0.0,>=3.10.10->promptlayer)\n",
      "  Downloading propcache-0.4.1-cp313-cp313-win_amd64.whl.metadata (14 kB)\n",
      "Collecting yarl<2.0,>=1.17.0 (from aiohttp<4.0.0,>=3.10.10->promptlayer)\n",
      "  Downloading yarl-1.22.0-cp313-cp313-win_amd64.whl.metadata (77 kB)\n",
      "Collecting hyperframe<7,>=6.1 (from h2<5.0.0,>=4.1.0->ably<3.0.0,>=2.0.11->promptlayer)\n",
      "  Using cached hyperframe-6.1.0-py3-none-any.whl.metadata (4.3 kB)\n",
      "Collecting hpack<5,>=4.1 (from h2<5.0.0,>=4.1.0->ably<3.0.0,>=2.0.11->promptlayer)\n",
      "  Using cached hpack-4.1.0-py3-none-any.whl.metadata (4.6 kB)\n",
      "Requirement already satisfied: anyio in c:\\users\\yue ning\\desktop\\misclearning\\ai_researcher_database\\venv\\lib\\site-packages (from httpx<0.29.0,>=0.28.1->promptlayer) (4.11.0)\n",
      "Requirement already satisfied: certifi in c:\\users\\yue ning\\desktop\\misclearning\\ai_researcher_database\\venv\\lib\\site-packages (from httpx<0.29.0,>=0.28.1->promptlayer) (2025.10.5)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\yue ning\\desktop\\misclearning\\ai_researcher_database\\venv\\lib\\site-packages (from httpx<0.29.0,>=0.28.1->promptlayer) (1.0.9)\n",
      "Requirement already satisfied: idna in c:\\users\\yue ning\\desktop\\misclearning\\ai_researcher_database\\venv\\lib\\site-packages (from httpx<0.29.0,>=0.28.1->promptlayer) (3.11)\n",
      "Requirement already satisfied: h11>=0.16 in c:\\users\\yue ning\\desktop\\misclearning\\ai_researcher_database\\venv\\lib\\site-packages (from httpcore==1.*->httpx<0.29.0,>=0.28.1->promptlayer) (0.16.0)\n",
      "Collecting importlib-metadata<8.8.0,>=6.0 (from opentelemetry-api<2.0.0,>=1.26.0->promptlayer)\n",
      "  Using cached importlib_metadata-8.7.0-py3-none-any.whl.metadata (4.8 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.5.0 in c:\\users\\yue ning\\desktop\\misclearning\\ai_researcher_database\\venv\\lib\\site-packages (from opentelemetry-api<2.0.0,>=1.26.0->promptlayer) (4.15.0)\n",
      "Collecting zipp>=3.20 (from importlib-metadata<8.8.0,>=6.0->opentelemetry-api<2.0.0,>=1.26.0->promptlayer)\n",
      "  Using cached zipp-3.23.0-py3-none-any.whl.metadata (3.6 kB)\n",
      "Collecting opentelemetry-semantic-conventions==0.59b0 (from opentelemetry-sdk<2.0.0,>=1.26.0->promptlayer)\n",
      "  Downloading opentelemetry_semantic_conventions-0.59b0-py3-none-any.whl.metadata (2.4 kB)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\yue ning\\desktop\\misclearning\\ai_researcher_database\\venv\\lib\\site-packages (from requests<3.0.0,>=2.31.0->promptlayer) (3.4.4)\n",
      "Collecting urllib3<3,>=1.21.1 (from requests<3.0.0,>=2.31.0->promptlayer)\n",
      "  Downloading urllib3-2.5.0-py3-none-any.whl.metadata (6.5 kB)\n",
      "Requirement already satisfied: sniffio>=1.1 in c:\\users\\yue ning\\desktop\\misclearning\\ai_researcher_database\\venv\\lib\\site-packages (from anyio->httpx<0.29.0,>=0.28.1->promptlayer) (1.3.1)\n",
      "Downloading promptlayer-1.0.71-py3-none-any.whl (38 kB)\n",
      "Downloading ably-2.1.1-py3-none-any.whl (128 kB)\n",
      "Downloading aiohttp-3.13.1-cp313-cp313-win_amd64.whl (450 kB)\n",
      "Using cached h2-4.3.0-py3-none-any.whl (61 kB)\n",
      "Using cached hpack-4.1.0-py3-none-any.whl (34 kB)\n",
      "Using cached hyperframe-6.1.0-py3-none-any.whl (13 kB)\n",
      "Downloading methoddispatch-3.0.2-py2.py3-none-any.whl (15 kB)\n",
      "Downloading msgpack-1.1.2-cp313-cp313-win_amd64.whl (72 kB)\n",
      "Downloading multidict-6.7.0-cp313-cp313-win_amd64.whl (45 kB)\n",
      "Downloading opentelemetry_api-1.38.0-py3-none-any.whl (65 kB)\n",
      "Using cached importlib_metadata-8.7.0-py3-none-any.whl (27 kB)\n",
      "Downloading opentelemetry_sdk-1.38.0-py3-none-any.whl (132 kB)\n",
      "Downloading opentelemetry_semantic_conventions-0.59b0-py3-none-any.whl (207 kB)\n",
      "Downloading pyee-12.1.1-py3-none-any.whl (15 kB)\n",
      "Downloading requests-2.32.5-py3-none-any.whl (64 kB)\n",
      "Downloading urllib3-2.5.0-py3-none-any.whl (129 kB)\n",
      "Using cached websockets-15.0.1-cp313-cp313-win_amd64.whl (176 kB)\n",
      "Downloading yarl-1.22.0-cp313-cp313-win_amd64.whl (86 kB)\n",
      "Downloading aiohappyeyeballs-2.6.1-py3-none-any.whl (15 kB)\n",
      "Downloading aiosignal-1.4.0-py3-none-any.whl (7.5 kB)\n",
      "Downloading attrs-25.4.0-py3-none-any.whl (67 kB)\n",
      "Downloading frozenlist-1.8.0-cp313-cp313-win_amd64.whl (43 kB)\n",
      "Downloading propcache-0.4.1-cp313-cp313-win_amd64.whl (40 kB)\n",
      "Using cached zipp-3.23.0-py3-none-any.whl (10 kB)\n",
      "Installing collected packages: methoddispatch, zipp, websockets, urllib3, pyee, propcache, multidict, msgpack, hyperframe, hpack, frozenlist, attrs, aiohappyeyeballs, yarl, requests, importlib-metadata, h2, aiosignal, opentelemetry-api, aiohttp, ably, opentelemetry-semantic-conventions, opentelemetry-sdk, promptlayer\n",
      "\n",
      "   - --------------------------------------  1/24 [zipp]\n",
      "   - --------------------------------------  1/24 [zipp]\n",
      "   --- ------------------------------------  2/24 [websockets]\n",
      "   --- ------------------------------------  2/24 [websockets]\n",
      "   --- ------------------------------------  2/24 [websockets]\n",
      "   --- ------------------------------------  2/24 [websockets]\n",
      "   --- ------------------------------------  2/24 [websockets]\n",
      "   --- ------------------------------------  2/24 [websockets]\n",
      "   ----- ----------------------------------  3/24 [urllib3]\n",
      "   ----- ----------------------------------  3/24 [urllib3]\n",
      "   ----- ----------------------------------  3/24 [urllib3]\n",
      "   ----- ----------------------------------  3/24 [urllib3]\n",
      "   ----- ----------------------------------  3/24 [urllib3]\n",
      "   ----- ----------------------------------  3/24 [urllib3]\n",
      "   ----- ----------------------------------  3/24 [urllib3]\n",
      "   ------ ---------------------------------  4/24 [pyee]\n",
      "   -------- -------------------------------  5/24 [propcache]\n",
      "   ---------- -----------------------------  6/24 [multidict]\n",
      "   ---------- -----------------------------  6/24 [multidict]\n",
      "   ----------- ----------------------------  7/24 [msgpack]\n",
      "   ------------- --------------------------  8/24 [hyperframe]\n",
      "   --------------- ------------------------  9/24 [hpack]\n",
      "   ---------------- ----------------------- 10/24 [frozenlist]\n",
      "   ------------------ --------------------- 11/24 [attrs]\n",
      "   ------------------ --------------------- 11/24 [attrs]\n",
      "   ------------------ --------------------- 11/24 [attrs]\n",
      "   ------------------ --------------------- 11/24 [attrs]\n",
      "   -------------------- ------------------- 12/24 [aiohappyeyeballs]\n",
      "   --------------------- ------------------ 13/24 [yarl]\n",
      "   --------------------- ------------------ 13/24 [yarl]\n",
      "   ----------------------- ---------------- 14/24 [requests]\n",
      "   ----------------------- ---------------- 14/24 [requests]\n",
      "   ----------------------- ---------------- 14/24 [requests]\n",
      "   ------------------------- -------------- 15/24 [importlib-metadata]\n",
      "   ------------------------- -------------- 15/24 [importlib-metadata]\n",
      "   -------------------------- ------------- 16/24 [h2]\n",
      "   -------------------------- ------------- 16/24 [h2]\n",
      "   ------------------------------ --------- 18/24 [opentelemetry-api]\n",
      "   ------------------------------ --------- 18/24 [opentelemetry-api]\n",
      "   ------------------------------ --------- 18/24 [opentelemetry-api]\n",
      "   ------------------------------ --------- 18/24 [opentelemetry-api]\n",
      "   ------------------------------ --------- 18/24 [opentelemetry-api]\n",
      "   ------------------------------ --------- 18/24 [opentelemetry-api]\n",
      "   ------------------------------- -------- 19/24 [aiohttp]\n",
      "   ------------------------------- -------- 19/24 [aiohttp]\n",
      "   ------------------------------- -------- 19/24 [aiohttp]\n",
      "   ------------------------------- -------- 19/24 [aiohttp]\n",
      "   ------------------------------- -------- 19/24 [aiohttp]\n",
      "   ------------------------------- -------- 19/24 [aiohttp]\n",
      "   ------------------------------- -------- 19/24 [aiohttp]\n",
      "   ------------------------------- -------- 19/24 [aiohttp]\n",
      "   ------------------------------- -------- 19/24 [aiohttp]\n",
      "   ------------------------------- -------- 19/24 [aiohttp]\n",
      "   ------------------------------- -------- 19/24 [aiohttp]\n",
      "   ------------------------------- -------- 19/24 [aiohttp]\n",
      "   ------------------------------- -------- 19/24 [aiohttp]\n",
      "   ------------------------------- -------- 19/24 [aiohttp]\n",
      "   ------------------------------- -------- 19/24 [aiohttp]\n",
      "   --------------------------------- ------ 20/24 [ably]\n",
      "   --------------------------------- ------ 20/24 [ably]\n",
      "   --------------------------------- ------ 20/24 [ably]\n",
      "   --------------------------------- ------ 20/24 [ably]\n",
      "   --------------------------------- ------ 20/24 [ably]\n",
      "   --------------------------------- ------ 20/24 [ably]\n",
      "   --------------------------------- ------ 20/24 [ably]\n",
      "   --------------------------------- ------ 20/24 [ably]\n",
      "   --------------------------------- ------ 20/24 [ably]\n",
      "   --------------------------------- ------ 20/24 [ably]\n",
      "   --------------------------------- ------ 20/24 [ably]\n",
      "   --------------------------------- ------ 20/24 [ably]\n",
      "   --------------------------------- ------ 20/24 [ably]\n",
      "   --------------------------------- ------ 20/24 [ably]\n",
      "   --------------------------------- ------ 20/24 [ably]\n",
      "   --------------------------------- ------ 20/24 [ably]\n",
      "   --------------------------------- ------ 20/24 [ably]\n",
      "   --------------------------------- ------ 20/24 [ably]\n",
      "   ---------------------------- ---- 21/24 [opentelemetry-semantic-conventions]\n",
      "   ---------------------------- ---- 21/24 [opentelemetry-semantic-conventions]\n",
      "   ---------------------------- ---- 21/24 [opentelemetry-semantic-conventions]\n",
      "   ---------------------------- ---- 21/24 [opentelemetry-semantic-conventions]\n",
      "   ---------------------------- ---- 21/24 [opentelemetry-semantic-conventions]\n",
      "   ---------------------------- ---- 21/24 [opentelemetry-semantic-conventions]\n",
      "   ---------------------------- ---- 21/24 [opentelemetry-semantic-conventions]\n",
      "   ---------------------------- ---- 21/24 [opentelemetry-semantic-conventions]\n",
      "   ---------------------------- ---- 21/24 [opentelemetry-semantic-conventions]\n",
      "   ---------------------------- ---- 21/24 [opentelemetry-semantic-conventions]\n",
      "   ---------------------------- ---- 21/24 [opentelemetry-semantic-conventions]\n",
      "   ---------------------------- ---- 21/24 [opentelemetry-semantic-conventions]\n",
      "   ---------------------------- ---- 21/24 [opentelemetry-semantic-conventions]\n",
      "   ---------------------------- ---- 21/24 [opentelemetry-semantic-conventions]\n",
      "   ---------------------------- ---- 21/24 [opentelemetry-semantic-conventions]\n",
      "   ------------------------------------ --- 22/24 [opentelemetry-sdk]\n",
      "   ------------------------------------ --- 22/24 [opentelemetry-sdk]\n",
      "   ------------------------------------ --- 22/24 [opentelemetry-sdk]\n",
      "   ------------------------------------ --- 22/24 [opentelemetry-sdk]\n",
      "   ------------------------------------ --- 22/24 [opentelemetry-sdk]\n",
      "   ------------------------------------ --- 22/24 [opentelemetry-sdk]\n",
      "   ------------------------------------ --- 22/24 [opentelemetry-sdk]\n",
      "   ------------------------------------ --- 22/24 [opentelemetry-sdk]\n",
      "   ------------------------------------ --- 22/24 [opentelemetry-sdk]\n",
      "   ------------------------------------ --- 22/24 [opentelemetry-sdk]\n",
      "   -------------------------------------- - 23/24 [promptlayer]\n",
      "   -------------------------------------- - 23/24 [promptlayer]\n",
      "   -------------------------------------- - 23/24 [promptlayer]\n",
      "   -------------------------------------- - 23/24 [promptlayer]\n",
      "   -------------------------------------- - 23/24 [promptlayer]\n",
      "   ---------------------------------------- 24/24 [promptlayer]\n",
      "\n",
      "Successfully installed ably-2.1.1 aiohappyeyeballs-2.6.1 aiohttp-3.13.1 aiosignal-1.4.0 attrs-25.4.0 frozenlist-1.8.0 h2-4.3.0 hpack-4.1.0 hyperframe-6.1.0 importlib-metadata-8.7.0 methoddispatch-3.0.2 msgpack-1.1.2 multidict-6.7.0 opentelemetry-api-1.38.0 opentelemetry-sdk-1.38.0 opentelemetry-semantic-conventions-0.59b0 promptlayer-1.0.71 propcache-0.4.1 pyee-12.1.1 requests-2.32.5 urllib3-2.5.0 websockets-15.0.1 yarl-1.22.0 zipp-3.23.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Collecting langchain\n",
      "  Downloading langchain-1.0.2-py3-none-any.whl.metadata (4.7 kB)\n",
      "Collecting langchain-core<2.0.0,>=1.0.0 (from langchain)\n",
      "  Downloading langchain_core-1.0.0-py3-none-any.whl.metadata (3.4 kB)\n",
      "Collecting langgraph<1.1.0,>=1.0.0 (from langchain)\n",
      "  Downloading langgraph-1.0.1-py3-none-any.whl.metadata (7.4 kB)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in c:\\users\\yue ning\\desktop\\misclearning\\ai_researcher_database\\venv\\lib\\site-packages (from langchain) (2.12.3)\n",
      "Collecting jsonpatch<2.0.0,>=1.33.0 (from langchain-core<2.0.0,>=1.0.0->langchain)\n",
      "  Using cached jsonpatch-1.33-py2.py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting langsmith<1.0.0,>=0.3.45 (from langchain-core<2.0.0,>=1.0.0->langchain)\n",
      "  Downloading langsmith-0.4.37-py3-none-any.whl.metadata (14 kB)\n",
      "Requirement already satisfied: packaging<26.0.0,>=23.2.0 in c:\\users\\yue ning\\desktop\\misclearning\\ai_researcher_database\\venv\\lib\\site-packages (from langchain-core<2.0.0,>=1.0.0->langchain) (25.0)\n",
      "Collecting pyyaml<7.0.0,>=5.3.0 (from langchain-core<2.0.0,>=1.0.0->langchain)\n",
      "  Downloading pyyaml-6.0.3-cp313-cp313-win_amd64.whl.metadata (2.4 kB)\n",
      "Collecting tenacity!=8.4.0,<10.0.0,>=8.1.0 (from langchain-core<2.0.0,>=1.0.0->langchain)\n",
      "  Downloading tenacity-9.1.2-py3-none-any.whl.metadata (1.2 kB)\n",
      "Requirement already satisfied: typing-extensions<5.0.0,>=4.7.0 in c:\\users\\yue ning\\desktop\\misclearning\\ai_researcher_database\\venv\\lib\\site-packages (from langchain-core<2.0.0,>=1.0.0->langchain) (4.15.0)\n",
      "Collecting jsonpointer>=1.9 (from jsonpatch<2.0.0,>=1.33.0->langchain-core<2.0.0,>=1.0.0->langchain)\n",
      "  Using cached jsonpointer-3.0.0-py2.py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting langgraph-checkpoint<4.0.0,>=2.1.0 (from langgraph<1.1.0,>=1.0.0->langchain)\n",
      "  Downloading langgraph_checkpoint-3.0.0-py3-none-any.whl.metadata (4.2 kB)\n",
      "Collecting langgraph-prebuilt<1.1.0,>=1.0.0 (from langgraph<1.1.0,>=1.0.0->langchain)\n",
      "  Downloading langgraph_prebuilt-1.0.1-py3-none-any.whl.metadata (5.0 kB)\n",
      "Collecting langgraph-sdk<0.3.0,>=0.2.2 (from langgraph<1.1.0,>=1.0.0->langchain)\n",
      "  Using cached langgraph_sdk-0.2.9-py3-none-any.whl.metadata (1.5 kB)\n",
      "Collecting xxhash>=3.5.0 (from langgraph<1.1.0,>=1.0.0->langchain)\n",
      "  Downloading xxhash-3.6.0-cp313-cp313-win_amd64.whl.metadata (13 kB)\n",
      "Collecting ormsgpack>=1.10.0 (from langgraph-checkpoint<4.0.0,>=2.1.0->langgraph<1.1.0,>=1.0.0->langchain)\n",
      "  Downloading ormsgpack-1.11.0-cp313-cp313-win_amd64.whl.metadata (1.2 kB)\n",
      "Requirement already satisfied: httpx>=0.25.2 in c:\\users\\yue ning\\desktop\\misclearning\\ai_researcher_database\\venv\\lib\\site-packages (from langgraph-sdk<0.3.0,>=0.2.2->langgraph<1.1.0,>=1.0.0->langchain) (0.28.1)\n",
      "Collecting orjson>=3.10.1 (from langgraph-sdk<0.3.0,>=0.2.2->langgraph<1.1.0,>=1.0.0->langchain)\n",
      "  Downloading orjson-3.11.3-cp313-cp313-win_amd64.whl.metadata (43 kB)\n",
      "Collecting requests-toolbelt>=1.0.0 (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.0.0->langchain)\n",
      "  Using cached requests_toolbelt-1.0.0-py2.py3-none-any.whl.metadata (14 kB)\n",
      "Requirement already satisfied: requests>=2.0.0 in c:\\users\\yue ning\\desktop\\misclearning\\ai_researcher_database\\venv\\lib\\site-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.0.0->langchain) (2.32.5)\n",
      "Collecting zstandard>=0.23.0 (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.0.0->langchain)\n",
      "  Downloading zstandard-0.25.0-cp313-cp313-win_amd64.whl.metadata (3.3 kB)\n",
      "Requirement already satisfied: anyio in c:\\users\\yue ning\\desktop\\misclearning\\ai_researcher_database\\venv\\lib\\site-packages (from httpx>=0.25.2->langgraph-sdk<0.3.0,>=0.2.2->langgraph<1.1.0,>=1.0.0->langchain) (4.11.0)\n",
      "Requirement already satisfied: certifi in c:\\users\\yue ning\\desktop\\misclearning\\ai_researcher_database\\venv\\lib\\site-packages (from httpx>=0.25.2->langgraph-sdk<0.3.0,>=0.2.2->langgraph<1.1.0,>=1.0.0->langchain) (2025.10.5)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\yue ning\\desktop\\misclearning\\ai_researcher_database\\venv\\lib\\site-packages (from httpx>=0.25.2->langgraph-sdk<0.3.0,>=0.2.2->langgraph<1.1.0,>=1.0.0->langchain) (1.0.9)\n",
      "Requirement already satisfied: idna in c:\\users\\yue ning\\desktop\\misclearning\\ai_researcher_database\\venv\\lib\\site-packages (from httpx>=0.25.2->langgraph-sdk<0.3.0,>=0.2.2->langgraph<1.1.0,>=1.0.0->langchain) (3.11)\n",
      "Requirement already satisfied: h11>=0.16 in c:\\users\\yue ning\\desktop\\misclearning\\ai_researcher_database\\venv\\lib\\site-packages (from httpcore==1.*->httpx>=0.25.2->langgraph-sdk<0.3.0,>=0.2.2->langgraph<1.1.0,>=1.0.0->langchain) (0.16.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\yue ning\\desktop\\misclearning\\ai_researcher_database\\venv\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.41.4 in c:\\users\\yue ning\\desktop\\misclearning\\ai_researcher_database\\venv\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.41.4)\n",
      "Requirement already satisfied: typing-inspection>=0.4.2 in c:\\users\\yue ning\\desktop\\misclearning\\ai_researcher_database\\venv\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.4.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\yue ning\\desktop\\misclearning\\ai_researcher_database\\venv\\lib\\site-packages (from requests>=2.0.0->langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.0.0->langchain) (3.4.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\yue ning\\desktop\\misclearning\\ai_researcher_database\\venv\\lib\\site-packages (from requests>=2.0.0->langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.0.0->langchain) (2.5.0)\n",
      "Requirement already satisfied: sniffio>=1.1 in c:\\users\\yue ning\\desktop\\misclearning\\ai_researcher_database\\venv\\lib\\site-packages (from anyio->httpx>=0.25.2->langgraph-sdk<0.3.0,>=0.2.2->langgraph<1.1.0,>=1.0.0->langchain) (1.3.1)\n",
      "Downloading langchain-1.0.2-py3-none-any.whl (107 kB)\n",
      "Downloading langchain_core-1.0.0-py3-none-any.whl (467 kB)\n",
      "Using cached jsonpatch-1.33-py2.py3-none-any.whl (12 kB)\n",
      "Downloading langgraph-1.0.1-py3-none-any.whl (155 kB)\n",
      "Downloading langgraph_checkpoint-3.0.0-py3-none-any.whl (46 kB)\n",
      "Downloading langgraph_prebuilt-1.0.1-py3-none-any.whl (28 kB)\n",
      "Using cached langgraph_sdk-0.2.9-py3-none-any.whl (56 kB)\n",
      "Downloading langsmith-0.4.37-py3-none-any.whl (396 kB)\n",
      "Downloading pyyaml-6.0.3-cp313-cp313-win_amd64.whl (154 kB)\n",
      "Downloading tenacity-9.1.2-py3-none-any.whl (28 kB)\n",
      "Using cached jsonpointer-3.0.0-py2.py3-none-any.whl (7.6 kB)\n",
      "Downloading orjson-3.11.3-cp313-cp313-win_amd64.whl (131 kB)\n",
      "Downloading ormsgpack-1.11.0-cp313-cp313-win_amd64.whl (112 kB)\n",
      "Using cached requests_toolbelt-1.0.0-py2.py3-none-any.whl (54 kB)\n",
      "Downloading xxhash-3.6.0-cp313-cp313-win_amd64.whl (31 kB)\n",
      "Downloading zstandard-0.25.0-cp313-cp313-win_amd64.whl (506 kB)\n",
      "Installing collected packages: zstandard, xxhash, tenacity, pyyaml, ormsgpack, orjson, jsonpointer, requests-toolbelt, jsonpatch, langsmith, langgraph-sdk, langchain-core, langgraph-checkpoint, langgraph-prebuilt, langgraph, langchain\n",
      "\n",
      "   ----------------------------------------  0/16 [zstandard]\n",
      "   -- -------------------------------------  1/16 [xxhash]\n",
      "   ----- ----------------------------------  2/16 [tenacity]\n",
      "   ----- ----------------------------------  2/16 [tenacity]\n",
      "   ------- --------------------------------  3/16 [pyyaml]\n",
      "   ------- --------------------------------  3/16 [pyyaml]\n",
      "   ------- --------------------------------  3/16 [pyyaml]\n",
      "   ---------- -----------------------------  4/16 [ormsgpack]\n",
      "   ------------ ---------------------------  5/16 [orjson]\n",
      "   --------------- ------------------------  6/16 [jsonpointer]\n",
      "   ----------------- ----------------------  7/16 [requests-toolbelt]\n",
      "   ----------------- ----------------------  7/16 [requests-toolbelt]\n",
      "   ----------------- ----------------------  7/16 [requests-toolbelt]\n",
      "   ----------------- ----------------------  7/16 [requests-toolbelt]\n",
      "   ----------------- ----------------------  7/16 [requests-toolbelt]\n",
      "   -------------------- -------------------  8/16 [jsonpatch]\n",
      "   ---------------------- -----------------  9/16 [langsmith]\n",
      "   ---------------------- -----------------  9/16 [langsmith]\n",
      "   ---------------------- -----------------  9/16 [langsmith]\n",
      "   ---------------------- -----------------  9/16 [langsmith]\n",
      "   ---------------------- -----------------  9/16 [langsmith]\n",
      "   ---------------------- -----------------  9/16 [langsmith]\n",
      "   ---------------------- -----------------  9/16 [langsmith]\n",
      "   ---------------------- -----------------  9/16 [langsmith]\n",
      "   ---------------------- -----------------  9/16 [langsmith]\n",
      "   ---------------------- -----------------  9/16 [langsmith]\n",
      "   ---------------------- -----------------  9/16 [langsmith]\n",
      "   ------------------------- -------------- 10/16 [langgraph-sdk]\n",
      "   --------------------------- ------------ 11/16 [langchain-core]\n",
      "   --------------------------- ------------ 11/16 [langchain-core]\n",
      "   --------------------------- ------------ 11/16 [langchain-core]\n",
      "   --------------------------- ------------ 11/16 [langchain-core]\n",
      "   --------------------------- ------------ 11/16 [langchain-core]\n",
      "   --------------------------- ------------ 11/16 [langchain-core]\n",
      "   --------------------------- ------------ 11/16 [langchain-core]\n",
      "   --------------------------- ------------ 11/16 [langchain-core]\n",
      "   --------------------------- ------------ 11/16 [langchain-core]\n",
      "   --------------------------- ------------ 11/16 [langchain-core]\n",
      "   --------------------------- ------------ 11/16 [langchain-core]\n",
      "   --------------------------- ------------ 11/16 [langchain-core]\n",
      "   --------------------------- ------------ 11/16 [langchain-core]\n",
      "   --------------------------- ------------ 11/16 [langchain-core]\n",
      "   --------------------------- ------------ 11/16 [langchain-core]\n",
      "   --------------------------- ------------ 11/16 [langchain-core]\n",
      "   --------------------------- ------------ 11/16 [langchain-core]\n",
      "   --------------------------- ------------ 11/16 [langchain-core]\n",
      "   --------------------------- ------------ 11/16 [langchain-core]\n",
      "   --------------------------- ------------ 11/16 [langchain-core]\n",
      "   --------------------------- ------------ 11/16 [langchain-core]\n",
      "   --------------------------- ------------ 11/16 [langchain-core]\n",
      "   --------------------------- ------------ 11/16 [langchain-core]\n",
      "   --------------------------- ------------ 11/16 [langchain-core]\n",
      "   --------------------------- ------------ 11/16 [langchain-core]\n",
      "   ------------------------------ --------- 12/16 [langgraph-checkpoint]\n",
      "   ------------------------------ --------- 12/16 [langgraph-checkpoint]\n",
      "   -------------------------------- ------- 13/16 [langgraph-prebuilt]\n",
      "   -------------------------------- ------- 13/16 [langgraph-prebuilt]\n",
      "   ----------------------------------- ---- 14/16 [langgraph]\n",
      "   ----------------------------------- ---- 14/16 [langgraph]\n",
      "   ----------------------------------- ---- 14/16 [langgraph]\n",
      "   ----------------------------------- ---- 14/16 [langgraph]\n",
      "   ----------------------------------- ---- 14/16 [langgraph]\n",
      "   ----------------------------------- ---- 14/16 [langgraph]\n",
      "   ----------------------------------- ---- 14/16 [langgraph]\n",
      "   ----------------------------------- ---- 14/16 [langgraph]\n",
      "   ----------------------------------- ---- 14/16 [langgraph]\n",
      "   ----------------------------------- ---- 14/16 [langgraph]\n",
      "   ------------------------------------- -- 15/16 [langchain]\n",
      "   ------------------------------------- -- 15/16 [langchain]\n",
      "   ------------------------------------- -- 15/16 [langchain]\n",
      "   ------------------------------------- -- 15/16 [langchain]\n",
      "   ---------------------------------------- 16/16 [langchain]\n",
      "\n",
      "Successfully installed jsonpatch-1.33 jsonpointer-3.0.0 langchain-1.0.2 langchain-core-1.0.0 langgraph-1.0.1 langgraph-checkpoint-3.0.0 langgraph-prebuilt-1.0.1 langgraph-sdk-0.2.9 langsmith-0.4.37 orjson-3.11.3 ormsgpack-1.11.0 pyyaml-6.0.3 requests-toolbelt-1.0.0 tenacity-9.1.2 xxhash-3.6.0 zstandard-0.25.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# %pip install pandas\n",
    "# %pip install camelot\n",
    "%pip install camelot-py\n",
    "%pip install ghostscript\n",
    "\n",
    "# If you're using tabula instead of Camelot:\n",
    "# pip install tabula-py\n",
    "\n",
    "# Optional - PromptLayer for tracking prompts\n",
    "%pip install promptlayer\n",
    "\n",
    "# Optional - LangChain if you want templated prompts / chaining\n",
    "%pip install langchain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86c25a64",
   "metadata": {},
   "source": [
    "# More complicated pipeline \n",
    "- Extract text and tables\n",
    "- Chunking \n",
    "- Output JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ba385857",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'camelot'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 7\u001b[39m\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpd\u001b[39;00m\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mollama\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mcamelot\u001b[39;00m  \u001b[38;5;66;03m# for table extraction\u001b[39;00m\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mextract_tables_from_pdf\u001b[39m(pdf_path):\n\u001b[32m      9\u001b[39m     tables = []\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'camelot'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "import PyPDF2\n",
    "import pandas as pd\n",
    "import ollama\n",
    "import camelot  # for table extraction\n",
    "def extract_tables_from_pdf(pdf_path):\n",
    "    tables = []\n",
    "    # Camelot works with PDFs that have text-based tables\n",
    "    try:\n",
    "        camelot_tables = camelot.read_pdf(pdf_path, pages='all', flavor='stream')\n",
    "        for t in camelot_tables:\n",
    "            tables.append(t.df.to_dict(orient='records'))\n",
    "    except Exception as e:\n",
    "        print(f\"⚠️ Table extraction failed: {e}\")\n",
    "    return tables\n",
    "\n",
    "extract_tables_from_pdf(\"../../Resources/ml_model_cardio_disease_detection.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a7e5da6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "import PyPDF2\n",
    "import pandas as pd\n",
    "import ollama\n",
    "import camelot  # for table extraction\n",
    "\n",
    "# ---------------------------\n",
    "# 1️⃣ PDF Text Extraction\n",
    "# ---------------------------\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    text = \"\"\n",
    "    with open(pdf_path, \"rb\") as file:\n",
    "        reader = PyPDF2.PdfReader(file)\n",
    "        for page in reader.pages:\n",
    "            text += page.extract_text() or \"\"\n",
    "    return text\n",
    "\n",
    "# ---------------------------\n",
    "# 2️⃣ Table Extraction\n",
    "# ---------------------------\n",
    "def extract_tables_from_pdf(pdf_path):\n",
    "    tables = []\n",
    "    # Camelot works with PDFs that have text-based tables\n",
    "    try:\n",
    "        camelot_tables = camelot.read_pdf(pdf_path, pages='all', flavor='stream')\n",
    "        for t in camelot_tables:\n",
    "            tables.append(t.df.to_dict(orient='records'))\n",
    "    except Exception as e:\n",
    "        print(f\"⚠️ Table extraction failed: {e}\")\n",
    "    return tables\n",
    "\n",
    "# ---------------------------\n",
    "# 3️⃣ Chunk Text for Large PDFs\n",
    "# ---------------------------\n",
    "def chunk_text(text, chunk_size=2000):\n",
    "    return [text[i:i+chunk_size] for i in range(0, len(text), chunk_size)]\n",
    "\n",
    "# ---------------------------\n",
    "# 4️⃣ Call Mistral Small 3.2 via Ollama\n",
    "# ---------------------------\n",
    "def summarize_chunk(chunk_text, tables=None):\n",
    "    table_text = \"\"\n",
    "    if tables:\n",
    "        # Flatten tables into markdown-like string\n",
    "        for i, table in enumerate(tables):\n",
    "            df = pd.DataFrame(table)\n",
    "            table_text += f\"\\nTable {i+1}:\\n\"\n",
    "            table_text += df.to_csv(index=False, sep='|')\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "You are a strict JSON-only research assistant. You will be given text from a research paper and optional tables.\n",
    "\n",
    "Extract a detailed structured summary in **valid JSON only** using this schema:\n",
    "\n",
    "{{\n",
    "  \"title\": \"\",\n",
    "  \"authors\": \"\",\n",
    "  \"problem_statement\": \"\",\n",
    "  \"dataset\": {{\n",
    "      \"description\": \"\",\n",
    "      \"size\": \"\",\n",
    "      \"source\": \"\",\n",
    "      \"preprocessing\": \"\"\n",
    "  }},\n",
    "  \"models\": [\n",
    "      {{\n",
    "          \"name\": \"\",\n",
    "          \"type\": \"\",\n",
    "          \"architecture_details\": \"\",\n",
    "          \"hyperparameters\": \"\",\n",
    "          \"results\": {{\n",
    "              \"accuracy\": \"\",\n",
    "              \"f1_score\": \"\",\n",
    "              \"roc_auc\": \"\",\n",
    "              \"other_metrics\": {{}}\n",
    "          }}\n",
    "      }}\n",
    "  ],\n",
    "  \"results_summary\": \"\",\n",
    "  \"conclusion\": \"\",\n",
    "  \"keywords\": []\n",
    "}}\n",
    "\n",
    "--- PAPER TEXT START ---\n",
    "{chunk_text}\n",
    "--- PAPER TEXT END ---\n",
    "\n",
    "--- TABLES START ---\n",
    "{table_text}\n",
    "--- TABLES END ---\n",
    "    \"\"\"\n",
    "\n",
    "    response = ollama.chat(\n",
    "        model=\"mistral-small:latest\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a strict JSON-only research assistant. Output only valid JSON.\"},\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ]\n",
    "    )\n",
    "    return response[\"message\"][\"content\"]\n",
    "\n",
    "# ---------------------------\n",
    "# 5️⃣ Post-process and Fix JSON\n",
    "# ---------------------------\n",
    "def fix_json(raw_text):\n",
    "    # remove trailing commas before closing braces/brackets\n",
    "    cleaned = re.sub(r\",(\\s*[\\]}])\", r\"\\1\", raw_text)\n",
    "    if not cleaned.strip().startswith(\"{\"):\n",
    "        cleaned = \"{\" + cleaned\n",
    "    if not cleaned.strip().endswith(\"}\"):\n",
    "        cleaned = cleaned + \"}\"\n",
    "    return cleaned\n",
    "\n",
    "# ---------------------------\n",
    "# 6️⃣ Main Workflow\n",
    "# ---------------------------\n",
    "def summarize_paper(pdf_path):\n",
    "    # Extract text and tables\n",
    "    text = extract_text_from_pdf(pdf_path)\n",
    "    tables = extract_tables_from_pdf(pdf_path)\n",
    "\n",
    "    # Chunk the text\n",
    "    chunks = chunk_text(text, chunk_size=3000)\n",
    "    summaries = []\n",
    "\n",
    "    for chunk in chunks:\n",
    "        raw_summary = summarize_chunk(chunk, tables)\n",
    "        fixed_summary = fix_json(raw_summary)\n",
    "        try:\n",
    "            summaries.append(json.loads(fixed_summary))\n",
    "        except json.JSONDecodeError:\n",
    "            print(\"⚠️ Could not parse JSON for chunk. Raw output:\")\n",
    "            print(raw_summary)\n",
    "\n",
    "    # Merge summaries (simple concatenation for now)\n",
    "    final_summary = {\n",
    "        \"title\": summaries[0].get(\"title\", \"\") if summaries else \"\",\n",
    "        \"authors\": summaries[0].get(\"authors\", \"\") if summaries else \"\",\n",
    "        \"problem_statement\": \" \".join(s.get(\"problem_statement\", \"\") for s in summaries),\n",
    "        \"dataset\": summaries[0].get(\"dataset\", {}) if summaries else {},\n",
    "        \"models\": [m for s in summaries for m in s.get(\"models\", [])],\n",
    "        \"results_summary\": \" \".join(s.get(\"results_summary\", \"\") for s in summaries),\n",
    "        \"conclusion\": \" \".join(s.get(\"conclusion\", \"\") for s in summaries),\n",
    "        \"keywords\": list({k for s in summaries for k in s.get(\"keywords\", [])})\n",
    "    }\n",
    "\n",
    "    return final_summary\n",
    "\n",
    "# ---------------------------\n",
    "# 7️⃣ Run\n",
    "# ---------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    pdf_path = \"../../Resources/ml_model_cardio_disease_detection.pdf\"\n",
    "    summary = summarize_paper(pdf_path)\n",
    "    print(json.dumps(summary, indent=2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74b3dbd9",
   "metadata": {},
   "source": [
    "# Dolphin 3 (archived)\n",
    "This model (llama3) does not support multimodal inputs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83b7196e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "def ask_dolphin(prompt):\n",
    "    response = requests.post(\n",
    "        \"http://localhost:11434/api/generate\",\n",
    "        json={\"model\": \"dolphin3\", \"prompt\": prompt, \"stream\": False}\n",
    "    )\n",
    "    return response.json()[\"response\"]\n",
    "\n",
    "schema = \"\"\"\n",
    "You are an AI that outputs ONLY valid JSON. \n",
    "Schema example:\n",
    "{\n",
    "  \"title\": \"\",\n",
    "  \"summary\": \"\",\n",
    "  \"methodology\": {\n",
    "    \"datasets\": [],\n",
    "    \"architecture\": \"\",\n",
    "    \"training\": \"\"\n",
    "  },\n",
    "  \"results\": \"\",\n",
    "  \"citations\": []\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "paper_text = \"Paste your paper text here or load from file.\"\n",
    "\n",
    "prompt = f\"{schema}\\nSummarize the following paper:\\n{paper_text}\"\n",
    "\n",
    "response = ask_dolphin(prompt)\n",
    "\n",
    "try:\n",
    "    json_output = json.loads(response)\n",
    "    print(\"✅ Valid JSON:\", json_output)\n",
    "except json.JSONDecodeError:\n",
    "    print(\"❌ Invalid JSON received:\\n\", response)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
